{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        self.leaf_num = 0\n",
    "        self.is_leaf = False\n",
    "        self.leaf_pred = None\n",
    "        self.split_feature = None\n",
    "        self.feature_index = None\n",
    "        self.is_continuous = None\n",
    "        self.split_feature_val = None\n",
    "        self.child = {}\n",
    "        self.high = -1\n",
    "\n",
    "\n",
    "def gini(y):\n",
    "    '''\n",
    "    计算基尼值\n",
    "    输入：y\n",
    "    返回基尼值\n",
    "    '''\n",
    "    p = y.value_counts() / y.shape[0]\n",
    "    gini =  1 - np.sum(p ** 2)\n",
    "    return gini\n",
    "\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, criterion, max_high):\n",
    "        self.criterion = criterion\n",
    "        self.pruning = None\n",
    "        self.tree = None\n",
    "        self.features = None\n",
    "        self.is_continuous = None\n",
    "        self.max_high = max_high\n",
    "\n",
    "    def fit(self, X_train, y_train, is_continuous):\n",
    "        '''\n",
    "        拟合数据\n",
    "        输入：X, y\n",
    "        输出树\n",
    "        调用函数：pre_pruning(输入训练数据) \\post_pruning（输入验证数据） \\create_tree（输入训练数据）\n",
    "\n",
    "        '''\n",
    "        self.features = list(X_train.columns)\n",
    "        self.is_continuous = is_continuous\n",
    "        self.tree = self.create_tree(X_train, y_train)\n",
    "\n",
    "\n",
    "    def create_tree(self, X, y):\n",
    "        '''\n",
    "        生成树的关键函数（递归）\n",
    "        输入：X，y\n",
    "        返回树\n",
    "        调用函数：类node（生成节点，根、内点、叶节点） 、判断y的取值是否唯一 、判断X是否为空 、 y最多次数的取值 、 \n",
    "                选择最合适的特征切分choose_best_feature_to_split(输入X，y返回最合适的特征名和不纯度(连续特征的不纯度还需有最合适的切分位置))\n",
    "                若为特征的取值为离散值，对每个取值生成子树？（不会太多吗），调用create_tree(输入去掉特征的X，和y，满足特征值相等)\n",
    "                若特征的取值为连续值，按照切分位置将数据分为左子树和右子树\n",
    "        '''\n",
    "        tree = Node()\n",
    "        tree.leaf_num = 0\n",
    "        if y.nunique() == 1:\n",
    "            tree.is_leaf = True\n",
    "            tree.leaf_pred = y.values[0]\n",
    "            tree.leaf_num += 1\n",
    "            tree.high = 0\n",
    "            return tree\n",
    "        \n",
    "        if X.empty or tree.high > self.max_high or X.duplicated(keep=False).sum()==X.shape[0]:\n",
    "            tree.is_leaf = True\n",
    "            tree.leaf_pred = y.value_counts().index[0]\n",
    "            tree.leaf_num += 1\n",
    "            tree.high = 0\n",
    "            return tree\n",
    "\n",
    "        split_feature, impurity = self.choose_best_feature_to_split_gini(X, y)\n",
    "\n",
    "        tree.split_feature = split_feature\n",
    "        tree.feature_index = self.features.index(split_feature)\n",
    "        tree.impurity = impurity[0]\n",
    "\n",
    "        feature_values = X.loc[:, split_feature]\n",
    "\n",
    "        if self.is_continuous[tree.feature_index] == 1:\n",
    "            tree.is_continuous = True\n",
    "            tree.split_feature_val = impurity[1]\n",
    "            sub_X = X.copy().drop(split_feature, axis=1)\n",
    "            low_tree = '>= {:.3f}'.format(tree.split_feature_val)\n",
    "            high_tree = '< {:.3f}'.format(tree.split_feature_val)\n",
    "            low_rows = feature_values >= impurity[1]\n",
    "            high_rows = feature_values <impurity[1]\n",
    "            tree.child[high_tree] = self.create_tree(sub_X[high_rows],y[high_rows])\n",
    "            tree.child[low_tree] = self.create_tree(sub_X[low_rows], y[low_rows])\n",
    "            tree.leaf_num += tree.child[high_tree].leaf_num +tree.child[low_tree].leaf_num\n",
    "            tree.high = max(tree.child[high_tree].high, tree.child[low_tree].high)\n",
    "\n",
    "        elif self.is_continuous[tree.feature_index] == 0:\n",
    "            tree.is_continuous = False\n",
    "            high = -1\n",
    "            feature_values_unique = feature_values.unique()\n",
    "            sub_X = X.copy().drop(split_feature, axis=1)\n",
    "            for value in feature_values_unique:\n",
    "                tree.child[value] = self.create_tree(sub_X[feature_values == value], y[feature_values == value])\n",
    "                tree.leaf_num += tree.child[value].leaf_num\n",
    "                high = max(high, tree.child[value].high) \n",
    "            tree.high = high + 1\n",
    "        \n",
    "        return tree\n",
    "        \n",
    "\n",
    "    def choose_best_feature_to_split_gini(self, X, y):\n",
    "        '''\n",
    "        根据Gini选出最合适的切分特征\n",
    "        输入：X， y\n",
    "        返回最合适的切分特证名和不纯度gini指标（若为连续值还加上最好的切分值）\n",
    "        调用函数：计算gini值gini_index（输入X在某特征下的数据，y，是否为连续值，返回gini指数）\n",
    "        '''\n",
    "        best_gini = [float('inf')]\n",
    "        split_feature = None\n",
    "        features = X.columns\n",
    "        for feature in features:\n",
    "            is_continuous = self.is_continuous[self.features.index(feature)]\n",
    "            gini_index = self.get_gini_index(X[feature], y, is_continuous)\n",
    "            if gini_index[0] < best_gini[0]:\n",
    "                split_feature = feature\n",
    "                best_gini = gini_index\n",
    "        \n",
    "        return split_feature, best_gini\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gini_index(feature, y, is_continuous):\n",
    "        '''\n",
    "        计算基尼指数，若为连续值，选择基尼指数最小的点作为分割点\n",
    "        输入：各特征值，y\n",
    "        返回最小的gini（和分割点）\n",
    "        调用函数：计算gini值gini（输入y，返回gini值）\n",
    "        '''\n",
    "        m = feature.shape[0]\n",
    "        feature_values_unique = sorted(feature.unique())\n",
    "        if is_continuous:\n",
    "            split_points = [(feature_values_unique[i]+feature_values_unique[i+1])/2 for i in range(len(feature_values_unique)-1)] \n",
    "            best_gini = float('inf')\n",
    "            best_split_point = None\n",
    "            for split_point in split_points:\n",
    "                part_low = y[feature <= split_point]\n",
    "                part_high = y[feature > split_point]\n",
    "                gini_index = len(part_low) / m * gini(part_low) + len(part_high) / m * gini(part_high)\n",
    "\n",
    "                if gini_index < best_gini:\n",
    "                    best_gini = gini_index\n",
    "                    best_split_point = split_point\n",
    "            return [best_gini, best_split_point]\n",
    "\n",
    "        else:\n",
    "            best_gini = 0\n",
    "            for val in feature_values_unique:\n",
    "                part = y[feature == val]\n",
    "                best_gini += len(part) / m * gini(part)\n",
    "            return [best_gini]\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        预测\n",
    "        输入：X\n",
    "        返回X对应的预测结果\n",
    "        调用函数：一条数据一条数据预测predic_for_one(输入一条数据x，返回其预测结果)\n",
    "        用循环写，递归容易栈溢出\n",
    "        '''\n",
    "        return X.apply(self.predict_by_one, axis=1)\n",
    "\n",
    "    def predict_by_one(self, x):\n",
    "        '''\n",
    "        输入x\n",
    "        输出预测结果\n",
    "        '''\n",
    "        tree = self.tree\n",
    "        while not tree.is_leaf:\n",
    "            if tree.is_continuous:\n",
    "                if x[tree.feature_index] >= tree.split_feature_val:\n",
    "                    tree = tree.child['>= {:.3f}'.format(tree.split_feature_val)]\n",
    "                else:\n",
    "                    tree = tree.child['< {:.3f}'.format(tree.split_feature_val)]\n",
    "            else:\n",
    "                tree = tree.child[x[tree.feature_index]]\n",
    "        return tree.leaf_pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(df):\n",
    "    df1 = df.iloc[:,:-1].apply(lambda column:((column - column.min()) /( column.max() - column.min()) - 0.5))\n",
    "    return df1.join(df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('pima_indian.csv')\n",
    "data = normalize_feature(data)\n",
    "X_train = data.iloc[:600, :-1]\n",
    "y_train = data.iloc[:600, -1]\n",
    "X_test = data.iloc[601:, :-1]\n",
    "y_test = data.iloc[601:, -1]\n",
    "is_con = [1,1,1,1,1,1,1,1]\n",
    "dtree = DecisionTree('gini',3)\n",
    "dtree.fit(X_train, y_train, is_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cb128e3992b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_valid' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = dtree.predict(X_valid)\n",
    "print(np.sum(list(y_valid==predictions))/y_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(prediction, y):\n",
    "    pred_acc = prediction == y\n",
    "    TP = (pred_acc & y == 1).sum()\n",
    "    TN = pred_acc.sum() - TP\n",
    "    FN = (y == 1).sum() - TP\n",
    "    FP = (y == 0).sum() - TN\n",
    "    P = TP / (TP + FP)\n",
    "    R = TP / (TP + FN)\n",
    "    F1_score = 2 * P * R / (P + R)\n",
    "    print('precision: %f, recall: %f, f1_score: %f'%(P, R, F1_score))\n",
    "    return P, R, F1_score\n",
    "classification_report(predictions, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
